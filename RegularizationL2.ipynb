{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDLH6HAazEVc",
        "outputId": "579dc523-181a-41d4-92c8-937e59c886b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed ayarlama\n",
        "torch.manual_seed(190401008)\n",
        "# Batch size seçilir\n",
        "batch_size = 16\n",
        "\n",
        "# Veri setini yükleme\n",
        "trainData = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/cure_the_princess_train.csv\")\n",
        "\n",
        "# Veri setini yükleme\n",
        "testData = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/cure_the_princess_test.csv\")\n",
        "\n",
        "# Veri setini yükleme\n",
        "validationData = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/cure_the_princess_validation.csv\")\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# TensorDataset kullanarak train, test ve validation setlerini oluşturuyoruz\n",
        "train_inputs = trainData.drop('Cured', axis=1).values\n",
        "train_labels = trainData['Cured'].values\n",
        "train_dataset = TensorDataset(torch.tensor(train_inputs, dtype=torch.float), torch.tensor(train_labels, dtype=torch.long))\n",
        "\n",
        "val_inputs = validationData.drop('Cured', axis=1).values\n",
        "val_labels = validationData['Cured'].values\n",
        "val_dataset = TensorDataset(torch.tensor(val_inputs, dtype=torch.float), torch.tensor(val_labels, dtype=torch.long))\n",
        "\n",
        "test_inputs = testData.drop('Cured', axis=1).values\n",
        "test_labels = testData['Cured'].values\n",
        "test_dataset = TensorDataset(torch.tensor(test_inputs, dtype=torch.float), torch.tensor(test_labels, dtype=torch.long))\n",
        "\n",
        "\n",
        "# DataLoader kullanarak train, test ve validation setlerini batch'ler halinde modele veriyoruz\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "andLE--zzJh2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define MLP class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden1 = nn.Linear(input_size, hidden_size1) # 1st hidden layer\n",
        "        self.hidden2 = nn.Linear(hidden_size1, hidden_size2) # 2nd hidden layer\n",
        "        self.output = nn.Linear(hidden_size2, output_size) # output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.hidden1(x)) # Apply ReLU activation function in the 1st hidden layer\n",
        "        x = torch.relu(self.hidden2(x)) # Apply ReLU activation function in the 2nd hidden layer\n",
        "        x = torch.sigmoid(self.output(x)) # Apply sigmoid activation function in the output layer\n",
        "        return x\n",
        "\n",
        "input_size = len(trainData.columns) - 1 # Number of input features, excluding the target column\n",
        "hidden_size1 = 100 # Number of units in the 1st hidden layer\n",
        "hidden_size2 = 50 # Number of units in the 2nd hidden layer\n",
        "output_size = 2 # Number of output units (for binary classification)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Use GPU if available, else use CPU\n",
        "\n",
        "# Define the MLP model\n",
        "model = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
        "# Send the model to the device (GPU or CPU)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the Cross Entropy loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the SGD optimizer\n",
        "# We can adjust the learning rate to prevent overfitting\n",
        "learning_rate = 0.01\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "1vTPA9zIzLZf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "train_loss_list = []\n",
        "validation_loss_list = []\n",
        "patience=5\n",
        "best_val_loss = None  # define best_val_loss \n",
        "lastStop=0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Train the model\n",
        "    model.train()  # train mode on\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        \n",
        "        # Move data to device\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        ### l2 regulazition ####\n",
        "        l2_lambda = 0.001\n",
        "        l2_reg = torch.tensor(0.)\n",
        "        for param in model.parameters():\n",
        "            l2_reg += torch.norm(param, p=2)\n",
        "\n",
        "        loss += l2_lambda * l2_reg\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)        \n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Print loss every few iterations\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}],  Loss: {loss.item():.4f}\")\n",
        "        \n",
        "\n",
        "    train_loss_list.append(train_loss / len(train_loader.dataset))  # add list for figure\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = 100 * correct / total\n",
        "\n",
        "    # Evaluate the model on validation set every epoch\n",
        "    model.eval()\n",
        "    val_loss=0.0\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in val_loader:\n",
        "            \n",
        "            # Move data to device\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass and calculate accuracy\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            ### l2 regulazition ####\n",
        "            l2_lambda = 0.001\n",
        "            l2_reg = torch.tensor(0.)\n",
        "            for param in model.parameters():\n",
        "                l2_reg += torch.norm(param, p=2)\n",
        "\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)  # Collect Losses\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        validation_loss_list.append(val_loss / len(val_loader.dataset))\n",
        "        val_loss/= len(val_loader.dataset) # Calculate the average by dividing the losses by the number of data\n",
        "        val_acc = correct / total\n",
        "        # print(f\"Validation accuracy after epoch {epoch+1}: {val_acc:.4f}\")\n",
        "\n",
        "        #EarlyStopping\n",
        "        val_score=val_loss\n",
        "\n",
        "        if best_val_loss is None:\n",
        "          patience_counter=0\n",
        "          best_val_loss=val_score       # Keep patience length in memory\n",
        "          torch.save(model.state_dict(),\"checkpoint.pt\")\n",
        "        elif best_val_loss<val_score:     # patience counter\n",
        "          patience_counter+=1\n",
        "          print(\"Earlystopping Patience Counter\",patience_counter)\n",
        "          if patience_counter==patience:           \n",
        "            break\n",
        "        else:\n",
        "          best_val_loss=val_score\n",
        "          torch.save(model.state_dict(),\"checkpoint.pt\")        # keeps the best model\n",
        "          patience_counter=0        # reset patience after we get the best model\n",
        "\n",
        "        lastStop=epoch+1\n",
        "\n",
        "        #print(f\"Validation accuracy after epoch {epoch+1}: {val_acc:.4f}\")\n",
        "        #print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsaEOGlvzM7H",
        "outputId": "6411a8a0-db38-476f-901d-317a4c2e187f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Earlystopping Patience Counter 1\n",
            "Earlystopping Patience Counter 1\n",
            "Earlystopping Patience Counter 1\n",
            "Earlystopping Patience Counter 2\n",
            "Earlystopping Patience Counter 3\n",
            "Earlystopping Patience Counter 1\n",
            "Earlystopping Patience Counter 1\n",
            "Earlystopping Patience Counter 1\n",
            "Earlystopping Patience Counter 2\n",
            "Earlystopping Patience Counter 3\n",
            "Earlystopping Patience Counter 4\n",
            "Earlystopping Patience Counter 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
        "\n",
        "newModel = MLP(input_size, hidden_size1, hidden_size2, output_size).to(device)\n",
        "\n",
        "# define best model path\n",
        "model_path = '/content/checkpoint.pt'\n",
        "\n",
        "# upload best model\n",
        "newModel.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "newModel.eval()\n",
        "\n",
        "# Loop over the test set and make predictions\n",
        "with torch.no_grad():\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = newModel(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        true_labels += labels.cpu().numpy().tolist()\n",
        "        predicted_labels += predicted.cpu().numpy().tolist()\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels)\n",
        "recall = recall_score(true_labels, predicted_labels)\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-k9zaTSzOmz",
        "outputId": "2ff70075-3e4c-4496-d26a-a8e9d2d8ea1e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9158\n",
            "Precision: 0.9109\n",
            "Recall: 0.9227\n",
            "F1 Score: 0.9168\n"
          ]
        }
      ]
    }
  ]
}